{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Description (English):**\n",
        "This script handles the entire machine learning pipeline: data loading, preprocessing, feature engineering, training the customer churn prediction model, and then saving the trained model and data preprocessor (like ColumnTransformer) into .pkl files.\n",
        "Important Note: You do NOT run this file on your deployment server! You run it once (or whenever you need to retrain the model) in a development environment like Google Colab. After successful execution, you must manually download the generated final_churn_predictor.pkl and data_preprocessor.pkl files and then upload them to your GitHub repository along with other project files.\n",
        "\n",
        "# **الوصف (العربية):**\n",
        "يتولى هذا السكريبت خط أنابيب تعلم الآلة بالكامل: تحميل البيانات، المعالجة المسبقة، هندسة الميزات، تدريب نموذج التنبؤ بمغادرة العملاء، ثم حفظ النموذج المُدرب ومعالج البيانات المسبق (مثل ColumnTransformer) في ملفات بصيغة .pkl.\n",
        "ملاحظة هامة: أنت لا تقوم بتشغيل هذا الملف على خادم النشر الخاص بك! تقوم بتشغيله مرة واحدة فقط (أو كلما احتجت لإعادة تدريب النموذج) في بيئة تطوير مثل Google Colab. بعد التنفيذ الناجح، يجب عليك تنزيل ملفي final_churn_predictor.pkl و data_preprocessor.pkl اللذين تم إنشاؤهما يدويًا، ثم رفعهما إلى مستودع GitHub الخاص بك جنبًا إلى جنب مع ملفات المشروع الأخرى."
      ],
      "metadata": {
        "id": "iosQyYldYRsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# train_and_save_model.py\n",
        "\n",
        "# --- 0. Dependency Installation (Google Colab Specific) ---\n",
        "# These commands should be run in your Colab environment before running the rest of the code.\n",
        "# You typically don't include these in the final script uploaded to GitHub for deployment.\n",
        "# !pip install catboost\n",
        "# !pip install imbalanced-learn\n",
        "# !pip install shap\n",
        "# !pip install kagglehub[pandas-datasets]\n",
        "\n",
        "# --- 1. Import Essential Libraries ---\n",
        "# This section imports all the necessary Python libraries for data manipulation,\n",
        "# visualization (though plots are usually omitted in deployment scripts),\n",
        "# machine learning model building, and saving/loading assets.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # Used for visualization in Colab, can be removed for pure script\n",
        "import seaborn as sns          # Used for visualization in Colab, can be removed for pure script\n",
        "import joblib # Essential for saving and loading models\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Pandas display options for better visibility in development environments\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "print(\"Libraries loaded successfully for training!\")\n",
        "\n",
        "# --- 2. Data Loading and Initial Preprocessing ---\n",
        "# This part handles fetching the dataset, prioritizing KaggleHub, then GitHub, then local file.\n",
        "# It also performs initial cleaning like handling missing 'TotalCharges' and converting 'Churn' to numeric.\n",
        "df = None\n",
        "file_path_telco = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
        "try:\n",
        "    df = kagglehub.load_dataset(KaggleDatasetAdapter.PANDAS, \"blastchar/telco-customer-churn\", file_path_telco)\n",
        "    print(\"Dataset loaded successfully using KaggleHub (Telco Customer Churn)!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Telco dataset with KaggleHub: {e}\")\n",
        "    github_data_url = 'https://raw.githubusercontent.com/IBM/telco-customer-churn-extra-data/master/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
        "    try:\n",
        "        df = pd.read_csv(github_data_url)\n",
        "        print(\"Dataset loaded successfully from GitHub URL!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset from GitHub URL: {e}\")\n",
        "        try:\n",
        "            df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "            print(\"Dataset loaded successfully from local file!\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"CRITICAL ERROR: 'WA_Fn-UseC_-Telco-Customer-Churn.csv' not found.\")\n",
        "\n",
        "if df is None:\n",
        "    raise SystemExit(\"Dataset not loaded, stopping execution.\")\n",
        "\n",
        "print(\"\\n--- Initial Data Snapshot ---\")\n",
        "print(df.head())\n",
        "print(\"\\n--- Data Info ---\")\n",
        "df.info()\n",
        "print(\"\\n--- Missing Values Check ---\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Handle 'TotalCharges' missing values and type conversion\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\n",
        "df.drop('customerID', axis=1, inplace=True) # Drop unique ID column\n",
        "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0}) # Convert target variable to binary\n",
        "print(\"\\n'Churn' column converted to numeric.\")\n",
        "\n",
        "# --- 3. Exploratory Data Analysis (EDA) - Optional for Script ---\n",
        "# This section contains plotting code for EDA. It's useful in a Jupyter/Colab notebook\n",
        "# but can be omitted from a script that's only meant to train and save a model.\n",
        "# Visualizations help understand data but are not needed for model's runtime.\n",
        "\n",
        "# --- 4. Feature Engineering ---\n",
        "# New, informative features are created from existing ones to enhance model performance.\n",
        "# These engineered features must also be created in the API for new prediction requests.\n",
        "print(\"\\n--- Feature Engineering ---\")\n",
        "df['TotalServices'] = (df[['PhoneService', 'MultipleLines', 'InternetService',\n",
        "                           'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
        "                           'TechSupport', 'StreamingTV', 'StreamingMovies']] == 'Yes').sum(axis=1)\n",
        "df['MonthlyChargePerTenure'] = df['MonthlyCharges'] / (df['tenure'] + 1e-6)\n",
        "df['HasInternetService'] = df['InternetService'].apply(lambda x: 1 if x != 'No' else 0)\n",
        "df['HasMultipleLines'] = df['MultipleLines'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "df['IsSeniorCitizen_Married'] = df.apply(lambda row: 1 if row['SeniorCitizen'] == 1 and row['Partner'] == 'Yes' else 0, axis=1)\n",
        "print(\"\\n--- Features after Engineering ---\")\n",
        "print(df[['tenure', 'MonthlyCharges', 'TotalCharges', 'TotalServices',\n",
        "          'MonthlyChargePerTenure', 'HasInternetService', 'HasMultipleLines',\n",
        "          'SeniorCitizen', 'Partner', 'IsSeniorCitizen_Married', 'Churn']].head())\n",
        "\n",
        "\n",
        "# --- 5. Data Splitting and Imbalance Handling ---\n",
        "# Data is split into training and testing sets. A ColumnTransformer is set up\n",
        "# to preprocess (scale numerical, one-hot encode categorical) features.\n",
        "# SMOTE is applied to the training data to balance the 'Churn' classes.\n",
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn']\n",
        "\n",
        "numeric_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
        "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "print(f\"\\nOriginal X_train shape: {X_train.shape}\")\n",
        "\n",
        "# Fit and transform training data, only transform test data\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Get feature names after preprocessing for SHAP and correct DataFrame construction\n",
        "ohe_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n",
        "passthrough_features = [col for col in X.columns if col not in numeric_features + categorical_features] # Should be empty\n",
        "all_feature_names = numeric_features + list(ohe_feature_names) + passthrough_features\n",
        "\n",
        "X_train_processed_df = pd.DataFrame(X_train_processed, columns=all_feature_names, index=X_train.index)\n",
        "X_test_processed_df = pd.DataFrame(X_test_processed, columns=all_feature_names, index=X_test.index)\n",
        "\n",
        "# Apply SMOTE to resample the training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed_df, y_train)\n",
        "print(f\"\\nResampled X_train shape: {X_train_resampled.shape}\")\n",
        "\n",
        "# --- 6. Build and Train the Model ---\n",
        "# Here, a LightGBM model is trained. You can extend this to train multiple models\n",
        "# (XGBoost, CatBoost) and select the best one based on evaluation metrics.\n",
        "print(\"\\n--- Training LightGBM Model for Saving ---\")\n",
        "lgb_model = lgb.LGBMClassifier(objective='binary', random_state=42, n_estimators=500, learning_rate=0.05, num_leaves=31)\n",
        "lgb_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# --- 7. Save the Model and Preprocessor ---\n",
        "# This is the crucial step for deploying the model. The trained model and the preprocessor\n",
        "# (which transforms raw input data into the format the model expects) are saved as .pkl files.\n",
        "# These files will be loaded by the FastAPI application.\n",
        "print(\"\\n--- Saving Model and Preprocessor ---\")\n",
        "joblib.dump(lgb_model, 'final_churn_predictor.pkl')\n",
        "joblib.dump(preprocessor, 'data_preprocessor.pkl')\n",
        "print(\"Model and preprocessor saved successfully! Please download these .pkl files from Colab.\")\n",
        "\n",
        "print(\"\\n'train_and_save_model.py' script finished. Remember to download .pkl files.\")"
      ],
      "metadata": {
        "id": "2yRB7ZPqYdIp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}